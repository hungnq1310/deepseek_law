services:
  deepseek:
    image: presencesw/deepseek-r1-distill-llama-8b-gguf:24.12
    tty: true
    volumes:
      - ./llm_models:/models
    ports:
      - "1234:8000"
      - "1235:8001"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo -n '' > /dev/tcp/127.0.0.1/8000"]
      interval: 60s
      timeout: 2m
      retries: 2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    command: >
      bash -c """pip install torch 'gguf>=0.10' && tritonserver --model-repository=/models"""
