{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "# tokenizer.save_pretrained(\"./llm_models/vllm.tokenizer/1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trism import TritonModel\n",
    "\n",
    "model = TritonModel(\n",
    "    model=\"vllm.tokenizer\",\n",
    "    version=1,\n",
    "    url=\"localhost:1234\",\n",
    "    grpc=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'<\\xef\\xbd\\x9cbegin\\xe2\\x96\\x81of\\xe2\\x96\\x81sentence\\xef\\xbd\\x9c><\\xef\\xbd\\x9cUser\\xef\\xbd\\x9c>Hello, how are you?',\n",
       "       b'<\\xef\\xbd\\x9cbegin\\xe2\\x96\\x81of\\xe2\\x96\\x81sentence\\xef\\xbd\\x9c><\\xef\\xbd\\x9cUser\\xef\\xbd\\x9c> asdasdasvasv',\n",
       "       b'<\\xef\\xbd\\x9cbegin\\xe2\\x96\\x81of\\xe2\\x96\\x81sentence\\xef\\xbd\\x9c><\\xef\\xbd\\x9cUser\\xef\\xbd\\x9c>How old are you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "output = model.run(\n",
    "    [np.array([\"Hello, how are you?\",\" asdasdasvasv\", \"How old are you\"], dtype=np.object_)]\n",
    ")\n",
    "output['TEXT_TEMPLATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>Hello, how are you?\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ> asdasdasvasv\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>How old are you\n"
     ]
    }
   ],
   "source": [
    "for output in output['TEXT_TEMPLATE']:\n",
    "    text = output.decode(\"utf-8\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<trism.inout.Inout at 0x7fd0233ba140>,\n",
       " <trism.inout.Inout at 0x7fd0233b9c90>,\n",
       " <trism.inout.Inout at 0x7fd022fdfa90>,\n",
       " <trism.inout.Inout at 0x7fd022fdefb0>,\n",
       " <trism.inout.Inout at 0x7fd022fdffd0>,\n",
       " <trism.inout.Inout at 0x7fd022fdfd30>,\n",
       " <trism.inout.Inout at 0x7fd022fdf640>,\n",
       " <trism.inout.Inout at 0x7fd022fdf070>,\n",
       " <trism.inout.Inout at 0x7fd022fdfa00>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# vllm_model = TritonModel(\n",
    "#     model=\"vllm.model\",\n",
    "#     version=1,\n",
    "#     url=\"localhost:1234\",\n",
    "#     grpc=False,\n",
    "# )\n",
    "# vllm_model._inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Length': '0', 'Content-Type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tritonclient.http import InferenceServerClient\n",
    "\n",
    "client = InferenceServerClient(\"localhost:1234\", verbose=True)\n",
    "client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/vllm.model, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/json', 'Content-Length': '971'}>\n",
      "bytearray(b'{\"name\":\"vllm.model\",\"versions\":[\"1\"],\"platform\":\"vllm\",\"inputs\":[{\"name\":\"text_input\",\"datatype\":\"BYTES\",\"shape\":[1]},{\"name\":\"stream\",\"datatype\":\"BOOL\",\"shape\":[1]},{\"name\":\"sampling_parameters\",\"datatype\":\"BYTES\",\"shape\":[1]},{\"name\":\"exclude_input_in_output\",\"datatype\":\"BOOL\",\"shape\":[1]},{\"name\":\"return_finish_reason\",\"datatype\":\"BOOL\",\"shape\":[1]},{\"name\":\"return_cumulative_logprob\",\"datatype\":\"BOOL\",\"shape\":[1]},{\"name\":\"return_logprobs\",\"datatype\":\"BOOL\",\"shape\":[1]},{\"name\":\"return_num_input_tokens\",\"datatype\":\"BOOL\",\"shape\":[1]},{\"name\":\"return_num_output_tokens\",\"datatype\":\"BOOL\",\"shape\":[1]}],\"outputs\":[{\"name\":\"text_output\",\"datatype\":\"BYTES\",\"shape\":[-1]},{\"name\":\"finish_reason\",\"datatype\":\"BYTES\",\"shape\":[-1]},{\"name\":\"cumulative_logprob\",\"datatype\":\"FP32\",\"shape\":[-1]},{\"name\":\"logprobs\",\"datatype\":\"BYTES\",\"shape\":[-1]},{\"name\":\"num_input_tokens\",\"datatype\":\"UINT32\",\"shape\":[1]},{\"name\":\"num_output_tokens\",\"datatype\":\"UINT32\",\"shape\":[-1]}]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'vllm.model',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'vllm',\n",
       " 'inputs': [{'name': 'text_input', 'datatype': 'BYTES', 'shape': [1]},\n",
       "  {'name': 'stream', 'datatype': 'BOOL', 'shape': [1]},\n",
       "  {'name': 'sampling_parameters', 'datatype': 'BYTES', 'shape': [1]},\n",
       "  {'name': 'exclude_input_in_output', 'datatype': 'BOOL', 'shape': [1]},\n",
       "  {'name': 'return_finish_reason', 'datatype': 'BOOL', 'shape': [1]},\n",
       "  {'name': 'return_cumulative_logprob', 'datatype': 'BOOL', 'shape': [1]},\n",
       "  {'name': 'return_logprobs', 'datatype': 'BOOL', 'shape': [1]},\n",
       "  {'name': 'return_num_input_tokens', 'datatype': 'BOOL', 'shape': [1]},\n",
       "  {'name': 'return_num_output_tokens', 'datatype': 'BOOL', 'shape': [1]}],\n",
       " 'outputs': [{'name': 'text_output', 'datatype': 'BYTES', 'shape': [-1]},\n",
       "  {'name': 'finish_reason', 'datatype': 'BYTES', 'shape': [-1]},\n",
       "  {'name': 'cumulative_logprob', 'datatype': 'FP32', 'shape': [-1]},\n",
       "  {'name': 'logprobs', 'datatype': 'BYTES', 'shape': [-1]},\n",
       "  {'name': 'num_input_tokens', 'datatype': 'UINT32', 'shape': [1]},\n",
       "  {'name': 'num_output_tokens', 'datatype': 'UINT32', 'shape': [-1]}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_model_metadata(\"vllm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from trism import TritonLMModel\n",
    "\n",
    "pipeline = TritonLMModel(\n",
    "    model=\"vllm.ensemble\",\n",
    "    version=1,\n",
    "    url=\"localhost:1235\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "sampling_parameters = {\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Begin chain of thought ..."
     ]
    }
   ],
   "source": [
    "async for token in pipeline.run(\n",
    "    \"Hello, how are you?\",\n",
    "    sampling_parameters=sampling_parameters,\n",
    "    show_thinking=True,\n",
    "):\n",
    "    print(token) # Check output\n",
    "await pipeline._serverclient.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt-hung-10_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
